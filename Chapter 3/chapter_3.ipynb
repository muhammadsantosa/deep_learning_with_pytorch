{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Tensors\n",
    "\n",
    "Learning Outcomes:\n",
    "- Understand tensors and basic datastructure within PyTorch\n",
    "- Indexing and operating tensors\n",
    "- Interoperating with NumPy multidimensional arrays\n",
    "- Using GPU for computations and speed\n",
    "\n",
    "Summary:\n",
    "- Neural nets turn floating points into other floating point representations. Can be seen by humans and interpreted but the transition cant really be interpreted\n",
    "- Floating points stored in tensors\n",
    "- Tensors are multidimensional arrays, core datastructure in PyTorch\n",
    "- PyTorch includes all you need to deal with tensors\n",
    "- Serialised to disks and loaded back\n",
    "- Cna run on CPU and GPU\n",
    "- Trailing udnerscore to operate on tensors\n",
    "\n",
    "Things To Look Out For:\n",
    "- Creating and using tensors\n",
    "- Different tensor opperations\n",
    "\n",
    "### 3.1 - Floating Point Numbers\n",
    "\n",
    "- Tensors are the generalisation of vectors to an arbitrary amount of dimensions\n",
    "- Multidimensional array\n",
    "PyTorch uses NumPy, SciPy, Pandas, and Sci-kit learn\n",
    "\n",
    "### 3.2 - Tensors: Multidimensional Arrays\n",
    "- Lists to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1.0, 2.0, 1.0]\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(3) #creates a tensor of 3 ones\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2]\n",
    "a[2] = 2 #Changing values in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 2., 2., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The below example is for a triangle with the points as their 2D coordinates\n",
    "points = torch.zeros(6)\n",
    "points[0] = 4.0\n",
    "points[1] = 1.0\n",
    "points[2] = 5.0\n",
    "points[3] = 2.0\n",
    "points[4] = 2.0\n",
    "points[5] = 1.0\n",
    "points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The below is the same as the above\n",
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each coordinate would be like the following\n",
    "(float(points[0]), float(points[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points, points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method above is quite convoluted, can do the below\n",
    "points = torch.zeros(3, 2)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor([4., 1.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0, 1], points[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensors store data more efficiently and in one area as opposed to lists\n",
    "- Indexing methods below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 3.],\n",
       "         [2., 1.]]),\n",
       " tensor([[5., 3.],\n",
       "         [2., 1.]]),\n",
       " tensor([5., 2.]),\n",
       " tensor([[[4., 1.],\n",
       "          [5., 3.],\n",
       "          [2., 1.]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:], points[1:,:], points[1:,0], points[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.2764e+00, -1.8891e+00,  2.5663e+00,  7.8000e-01,  1.8298e-02],\n",
       "          [-9.5934e-02,  7.0364e-01,  1.3453e+00,  7.9081e-01,  8.1403e-01],\n",
       "          [-1.0623e+00, -1.2951e+00, -7.2420e-01, -1.1008e+00,  5.6799e-01],\n",
       "          [ 6.5098e-01,  3.3948e-01,  1.7156e-01, -2.9337e-01, -1.2991e-01],\n",
       "          [-5.5935e-01, -2.3687e-03, -3.4211e-01, -1.9345e+00,  8.6455e-02]],\n",
       " \n",
       "         [[ 1.7119e+00,  8.8456e-01,  1.9680e+00, -1.4753e+00,  6.5083e-01],\n",
       "          [ 1.0845e+00,  6.4771e-01, -4.9513e-01, -1.3086e+00, -6.6175e-02],\n",
       "          [ 6.9321e-02,  1.3540e-01, -5.7981e-01, -1.5813e-01, -1.0391e+00],\n",
       "          [ 3.2854e-01,  3.2642e-01, -9.5890e-01,  5.7654e-01,  2.0980e-02],\n",
       "          [-2.2071e-01,  8.7300e-01,  2.0059e+00,  1.3835e-01, -1.2173e-01]],\n",
       " \n",
       "         [[ 2.9886e-01,  1.4612e+00,  2.2837e-01,  6.0616e-01, -1.1273e+00],\n",
       "          [-9.4239e-01, -2.1020e+00,  1.0691e+00,  1.1302e-01,  3.2141e-01],\n",
       "          [-3.9510e-02,  4.8467e-01,  4.8397e-01, -7.1440e-01,  2.3843e+00],\n",
       "          [ 1.1056e+00,  7.7453e-02,  6.1576e-01, -1.5508e+00, -7.3167e-01],\n",
       "          [-1.0831e-01, -4.4851e-01,  1.3047e+00,  1.7594e+00,  3.0226e-01]]]),\n",
       " torch.Size([3, 5, 5]),\n",
       " tensor([0.2126, 0.7152, 0.0722]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5) #Channels, rows, columns, 3 dimensions?\n",
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "\n",
    "img_t, img_t.shape, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3139,  0.9065, -2.0794, -0.4127, -1.1388],\n",
       "         [-0.9820, -0.2699, -0.3858, -0.4354,  0.9286],\n",
       "         [-0.0916, -0.5251,  1.4709, -1.8182, -0.5707],\n",
       "         [-0.8443, -0.3540,  0.2046,  0.8712, -0.2835],\n",
       "         [ 0.0639,  0.6626,  0.4145, -0.5431,  0.0250]],\n",
       "\n",
       "        [[-0.2236, -1.5097, -0.9841, -1.1071, -0.6928],\n",
       "         [-0.4414, -0.2183,  0.0961,  1.4813, -1.7205],\n",
       "         [-0.4203,  0.6960, -0.2698,  1.3844,  1.0403],\n",
       "         [-1.5784,  0.8017,  0.1664, -1.1519, -0.3867],\n",
       "         [-0.0133, -0.3662, -2.2483, -0.2246,  1.6467]],\n",
       "\n",
       "        [[ 0.6792,  0.6136,  1.6586,  0.1744,  1.2825],\n",
       "         [ 0.4921, -3.1010, -0.7434, -1.5832,  1.0970],\n",
       "         [ 1.3099, -0.1467,  0.0486, -0.2931, -0.5044],\n",
       "         [ 1.4283,  0.9263, -0.3370, -1.1557, -0.1936],\n",
       "         [ 0.6499,  0.4755,  1.5617,  0.1628,  0.7567]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) #shape [batch, channels, rows, columns]\n",
    "batch_t[0] #Splits it into batches which can be easier for the data to be processed by the neural network\n",
    "#Batches are also really useful for use with GPU's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3) #Generalise based on the different shapes\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Broadcasting takes smaller tensors and changes the shapes so that you can do element wise operations between tensors\n",
    "unsqueezed_weights = weights.unsqueeze(-1).unsqueeze(-1)\n",
    "img_weights = (img_t * unsqueezed_weights)\n",
    "batch_weights = (batch_t * unsqueezed_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "batch_weights.shape, batch_t.shape, unsqueezed_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3902, -0.8427, -1.0262, -0.8669, -0.6450],\n",
       "         [-0.4890, -0.4374, -0.0670,  0.8526, -0.9539],\n",
       "         [-0.2255,  0.3756,  0.1233,  0.5824,  0.5863],\n",
       "         [-1.2053,  0.5650,  0.1382, -0.7221, -0.3508],\n",
       "         [ 0.0510, -0.0867, -1.4071, -0.2644,  1.2377]],\n",
       "\n",
       "        [[ 0.8797, -0.1352,  0.0755,  0.1206,  0.1605],\n",
       "         [ 1.6362,  0.5831,  0.1041, -0.6521, -0.0454],\n",
       "         [-0.1227, -0.2197, -0.6143, -0.0905,  0.3554],\n",
       "         [ 0.1721, -0.1028, -0.2085, -0.3397,  2.4509],\n",
       "         [-1.4506, -1.9501,  0.4921, -0.8770,  0.0469]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Einsum can do the same\n",
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/8tk4c03d6td9nktt82r_rb380000gn/T/ipykernel_37014/3551129751.py:2: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/c10/core/TensorImpl.h:1928.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"channels\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can name the dimensions\n",
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=[\"channels\"])\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named: torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named: torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "#Use refine names to not change existing ones\n",
    "img_named = img_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "batch_named = batch_named = batch_t.refine_names(..., \"channels\", \"rows\", \"columns\")\n",
    "print(\"img named:\", img_named.shape, img_named.names)\n",
    "print(\"batch named:\", batch_named.shape, batch_named.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Align_as takes care of the missing dimensions not taken care of\n",
    "weights_aligned = weights_named.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to sum on the channels dimensions\n",
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gray_named \u001b[38;5;241m=\u001b[39m (\u001b[43mimg_named\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights_named\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "gray_named = (img_named[...,:3] * weights_named).sum(\"channels\") #Cannot since they are of different dimensions and channels is not in the same locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names #names have been renamed to none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Must specify the data type in the tensors so that computations can be done efficiently\n",
    "- Most time spent on float32 and int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int16, torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can specify the datatype as an argument\n",
    "double_points = torch.ones(10, 2, dtype=torch.double)\n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
    "short_points.dtype, double_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_points = torch.zeros(10, 2).double()\n",
    "short_points = torch.ones(10, 2).short()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To as a converter\n",
    "double_points = torch.zeros(10, 2).to(torch.double)\n",
    "short_points = torch.ones(10, 2).to(dtype=torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Need to make sure if there is a computation between tensors\n",
    "points_64 = torch.rand(5, dtype=torch.double) #random tensor of size 5 containing numbers from 0 to 1\n",
    "points_short = points_64.to(dtype=torch.short)\n",
    "points_64 * points_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)\n",
    "a.shape, a_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different tensor opperations\n",
    "- Cteation ops - constructing tensors\n",
    "- Indexing, slicing, joining, mutating ops - Changing shape, stride or content\n",
    "- Math ops - pointwise, reduction, comparison, spectral\n",
    "\n",
    "Indexing into storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cy/8tk4c03d6td9nktt82r_rb380000gn/T/ipykernel_37014/3092447483.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points.storage() #This is how its stored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.storage() #This is how its stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "points_storage[0] #Storage layout is always 1D, cannot be 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_storage[0] = 2.0 #Changes the points.storage()\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In place operations include trailing __ that show that it replaces not makes a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a.zero_()\n",
    "a #Didnt have to assing a.zero_() to another variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "second_point.storage_offset() #Shows the number of storage elements before you get to the one you are on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.size(), second_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride() #Need to go 2 across and one down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not always desirable to change the original tensor, can clone\n",
    "second_point  = points[1].clone()\n",
    "second_point[0] = 10.0\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transposing without copying\n",
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing that two tensors share the same storage (i.e, not coppied)\n",
    "id(points.storage()) == id(points_t.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only differe in shape and stride\n",
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transposing in Higher Dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), (20, 5, 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a high dimensional t ensore of ones, and transposing to specified shape\n",
    "some_t = torch.ones(3, 4, 5)\n",
    "transpose_t = some_t.transpose(0, 2) #Flips the shape and the strid\n",
    "some_t.shape, some_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 3]), (1, 5, 20))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t.shape, transpose_t.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this improves the way that data is retrived making it more efficient\n",
    "\n",
    "##### Contiguous Tensors\n",
    "- Elements are laid out sequentially in memory, a lot more efficient as stated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.is_contiguous(), points_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 5., 2.],\n",
       "         [1., 3., 1.]]),\n",
       "  4.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       " (1, 2))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0],[5.0, 3.0], [2.0, 1.0]])\n",
    "points_t = points.t()\n",
    "points_t, points_t.storage(), points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 5., 2.],\n",
       "         [1., 3., 1.]]),\n",
       " (3, 1),\n",
       "  4.0\n",
       "  5.0\n",
       "  2.0\n",
       "  1.0\n",
       "  3.0\n",
       "  1.0\n",
       " [torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making it a contiguous tensor\n",
    "points_t_cont = points_t.contiguous() \n",
    "#Looking at the stride\n",
    "points_t_cont, points_t_cont.stride(), points_t_cont.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Moving tensors to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.to(device='mps') #Since apple doesnt support cuda, use the mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 2 * points\n",
    "points_gpu = 2 * points.to(device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_cpu = points_gpu.to(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NumPy Interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Turning pytorch tensor into a numpy array\n",
    "points = torch.ones(3, 4)\n",
    "points_np = points.numpy()\n",
    "points_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can turn a numpy array into a tensor\n",
    "points = torch.from_numpy(points_np)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can also save tensors to a file and open it later if its really important using the load function and save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
